% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode
% File acl-ijcnlp2009.tex
%
% Contact  jshin@csie.ncnu.edu.tw
%%
%% Based on the style files for EACL-2009 and IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl-ijcnlp2009}
\usepackage[	
   pdfdisplaydoctitle, breaklinks, colorlinks, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black, 
   backref, hyperfootnotes]{hyperref} % backref a modre URL asi nakonec zrusime 
%\usepackage{times}
\usepackage{url}
\usepackage{amsmath}
\usepackage{color} %pro korektury
\usepackage{paralist} % for better itemize and enumerate

% a footer required for the first page
\usepackage{fancyhdr}
\fancyhead{} % clear all header fields
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[C]{Proceedings of ICON-2009: 7th International Conference on Natural Language Processing, Macmillan Publishers, India. Also accessible from http://ltrc.iiit.ac.in/proceedings/ICON-2009}

% xelatex
\usepackage{fontspec, xunicode, xltxtra}
\defaultfontfeatures{Mapping=tex-text}
\setmainfont{Times New Roman}
\setmonofont[Scale=MatchLowercase]{Luxi Mono}
\setmathsf{Lohit Hindi}%\XXX
%\newfontinstance\hi[Script=Devanagari]{Lohit Hindi}
\newfontinstance\hifont[Script=Devanagari]{Code2000}
\newfontinstance\bnfont[Script=Bengali]{Code2000}
\newfontinstance\tefont[Script=Telugu]{Code2000}
\newfontinstance\translitfont{Gentium}
\newcommand{\hi}[1]{{\hifont #1}}
\newcommand{\bn}[1]{{\bnfont #1}}
\newcommand{\te}[1]{{\tefont #1}}
\newcommand{\translit}[1]{{\translitfont \textit{#1}}}

% natbib
\usepackage{natbib}
\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}

% our defs
\def\perscite#1{\citet{#1}}  
\def\parcite#1{\citep{#1}} 
%ps: Did you mean \citep and \citet (in-parentheses and textual reference)? There is even more, see `texdoc natbib`.
\def\Sref#1{Section~\ref{#1}}
\def\Tref#1{Table~\ref{#1}}
\def\Fref#1{Figure~\ref{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}} % komentare (TODO)
\newcommand{\XXX}{\textcolor{red}{XXX }} % komentare (TODO)

\def\microsection#1{{\bf #1.}}



\title{Maximum Spanning Malt: Hiring World's Leading Dependency Parsers to Plant Indian Trees%
% Tohle nechat zakomentované, je to jen tahák, jak udělat acknowledgement grantu při nedostatku místa. Jinak ale mám momentálně na konci opravdovou sekci Acknowledgements.
%\thanks{ \hspace{.6em}The research has been supported by the grant 
%MSM0021620838 (Czech Ministry of Education).}
}

% Tady je posuzování taky slepé? Až odtajním autora, nezapomenout odtajnit i acknowledgements!
\author{%Daniel Zeman\\
%Univerzita Karlova v Praze, Ústav formální a aplikované lingvistiky\\
%Malostranské náměstí 25, CZ-11800, Praha, Czechia\\ 
%\texttt{zeman@ufal.mff.cuni.cz}
}

%\title{Instructions for ACL-IJCNLP 2009 Proceedings}
%
%\author{First Author\\
%  Affiliation / Address line 1\\
%  Affiliation / Address line 2\\
%  {\tt email@domain}  \And
%  Second Author\\
%  Affiliation / Address line 1\\
%  Affiliation / Address line 2\\
%  {\tt  email@domain}}

\date{}

\begin{document}
\maketitle
\thispagestyle{fancy}

\begin{abstract}
We present our system used for participation in the ICON 2009 NLP Tools Contest: dependency parsing of Hindi, Bangla and Telugu. The system consists of three existing, freely available dependency parsers, two of which (MST and Malt) have been known to produce state-of-the-art structures on data sets for other languages. Various settings of the parsers are explored in order to adjust them for the three Indian languages, and a voting approach is used to combine them into a superparser. Since there is nothing novel about the approach used, substantial part of the paper is devoted to the analysis of errors the system makes on the given data sets.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Dependency parsing, i.e. sentence analysis that outputs tree of word-on-word dependencies (as opposed to constituent trees of context-free derivations), gained growing attention and popularity recently. There are data-driven dependency parsers that can be trained on syntactically annotated corpora (treebanks) and new, previously unseen material can be parsed very efficiently \citep{nivre:2009:ACLIJCNLP}.

Most of the successful parsers employ discriminative learning techniques to sort out vast sets of potentially useful features observed in the input text. Thus, for every new training treebank, smart feature engineering is the key to getting the most out of the existing parsers, regardless how well they performed on other data sets and languages. Now that there are new treebanks available for two Indo-Aryan and one Dravidian language, we took three existing dependency parsers and explored the possibilities of tuning them for the new training data. Both parser configuration and data preprocessing are relevant approaches to the tuning. In addition, we used parser combination to further improve the results.

Throughout the paper we focus mainly on the unlabeled attachment score. Although the parsers produce labeled dependencies, we do not optimize the system towards label accuracy.

The rest of the paper is organized as follows: In \Sref{sec:system}, we describe the respective parsers and the combined parsing system. In \Sref{sec:experiments}, we report on the experiments we performed, discuss various results on the development set and analyze the errors. We conclude by summarizing the best configuration we were able to find, and future implications.

\section{System Description}
\label{sec:system}

Several good trainable dependency parsers have emerged during the past five years. The CoNLL-X \citep{buchholz-marsi:2006:CoNLL-X} and CoNLL 2007 \citep{nivre-EtAl:2007:EMNLP-CoNLL2007} shared tasks in multilingual dependency parsing have greatly contributed to the development of the parsers. Some of the parsers are now freely available on the web, some are even open-source. We selected three of the publicly available parsers for our experiments:

%\microsection{MST Parser}
\subsection{MST Parser}
\label{sec:mst}
The Maximum Spanning Tree (MST) parser \citep{mst} views the sentence as an oriented complete graph with edges weighted by a feature scoring function. It finds for the graph a spanning tree that maximizes the weights of the edges. A multi-class classification algorithm called MIRA is used to compute the scoring function.

MST Parser achieved the best unlabeled attachment scores (UAS) for 9 out of the 13 languages of CoNLL-X, and second best scores in two others. Parsing is fast but training the parser takes many hours on large treebanks. On small data however, multiple quick experiments with different settings are still doable. The parser is implemented in Java and freely available for download.\footnote{\url{http://sourceforge.net/projects/mstparser/}}

%\microsection{Malt Parser}
\subsection{Malt Parser}
\label{sec:malt}
The Malt Parser  \citep{malt} is a deterministic shift-reduce parser where input words can be either put to the stack or taken from the stack and combined to form a dependency. The decision, which operation to perform, is made by an oracle based on various features of the words in the input buffer and the stack. The default machine learning algorithm used to train the oracle is a sort of SVN (support vector machine) classifier \citep{svm}.

Malt Parser has participated in both CoNLL-X and CoNLL 2007 shared tasks, and although it achieved the best UAS in three languages only, it usually scored among the five best parsers, sometimes with statistically insignificant difference from the winner. Malt Parser is really fast and its new Java implementation is open-source, freely available for download.\footnote{\url{http://maltparser.org/}}

%\microsection{DZ Parser}
\subsection{DZ Parser}
\label{sec:dz}
In order to combine the two above parsers, we needed a third parser. We picked DZ Parser \citep{dzparser}, which is also reasonably fast and freely available.\footnote{\url{http://ufal.mff.cuni.cz/~zeman/projekty/parser/}} Although its accuracy, if compared to MST or Malt, is worse by a wide margin, this parser proved useful because its only role was to help to form a majority whenever MST and Malt disagreed.

DZ Parser builds a model of bigrams of words that occur together in a dependency; most of the time, words are identified by their part of speech tags and morphological features. The parser was originally developed for Czech but it can be re-trained for any other language.\footnote{Of course there are other dependency parsers that successfully participated in the CoNLL shared tasks and are available for download. One alternative worth mentioning is the ISBN Parser \citep{titov-henderson:2007:EMNLP-CoNLL2007} at \url{http://flake.cs.uiuc.edu/~titov/}.}

\subsection{Voting Superparser}
\label{sec:voting}
The three parsers are combined using a simple weighted-voting approach similar to \citet{biblio:ZeZaImprovingParsing2005}, except that the output is guaranteed to be cycle-free. We start by evaluating every parser separately on the development data. The UAS of each parser is subsequently used as the weight of that parser's vote. Dependencies are parent-child relations, and for every node there are up to three candidates for its parent (if all three parsers disagree). Candidates get weighted votes -- e.g., if parsers with weights $w_1 = 0.8$ and $w_2 = 0.7$ agree on the candidate, the candidate gets 1.5 votes. Since we have only three parsers, in practice this means that the candidate of the best parser looses only if 1. the other two parsers agree on someone else, or 2. if attaching the child to this candidate would create a cycle.

The tree is constructed from the root down. We repeatedly add nodes whose winning parent candidates are already in the tree. If none of the remaining nodes meet this condition, we have to break a cycle. We do so by removing the candidate (for parentship of any remaining node) with least weighted votes. Then we go on with adding nodes until all nodes are attached or there is another cycle to break.

\section{Experiments}
\label{sec:experiments}

blabla

\begin{compactitem}
\item \hi{स्टैंडर्डज} \textit{(sṭaiṁḍarḍaja)} \translit{(sṭaiṁḍarḍaja)}
\item \hi{स्टैंडर्डस} \textit{(sṭaiṁḍarḍasa)}
\item \hi{स्टैंडर्ड्स} \textit{(sṭaiṁḍarḍsa)}
\item \bn{সরিয়ে} Tohle je bengálsky.
\item \te{వాడుతున్నాం} Tohle je telugsky.
\end{compactitem}

\begin{table}[t]
\begin{center}
\small
\begin{tabular}{l  l | ll}
factor & BLEU &
factor & BLEU\\
\hline
tag & 12.03±0.75 &	hitbsuf & 11.58±0.74\\
wc50 & 11.97±0.73 &	hindomor2 & 11.55±0.74\\
wc10 & 11.76±0.74 &	hindomor1 & 11.54±0.71\\
lcsuf3 & 11.66±0.75 &	affddf & 11.50±0.7\\
lcsuf1 & 11.63±0.72 &	affbdf & 11.33±0.72\\
hindomor3 & 11.60±0.73 &	lcsuf2 & 11.14±0.74\\
\end{tabular}
\end{center}
\caption{Target side morphology: Using different additional factors for second
language model of MT system and its  effect on BLEU score. Trained on
IIIT-TIDES only. 
tag -- POS tags; wc\textit{n} -- \textit{n} word classes from mkcls; hitbsuf --
word classes created by hand; lcsuf\textit{n} -- simple
n-character suffixes; hindomor\textit{n};
aff\textit{xxx} -- Affisix
} 
\label{tab:morph}
\end{table}

\begin{table}[ht]
\begin{centering}
\begin{tabular}{l|l|l}
\textbf{Parallel data} & \textbf{Joshua} & \textbf{Moses} \\
\hline
Tides & 12.27±0.83 & 11.46±0.72\\
Tides+DP & \textbf{12.58±0.77} & 11.93±0.75\\
Tides+DP+Emille & 11.32±0.74 & 10.06±0.72\\
Tides+DP+Dict & 12.43±0.79 & 11.90±0.78\\
\end{tabular}
\caption{Results of Joshua compared with Moses}
\label{tab:joshua}
\end{centering}
\end{table}

\section{Parser Unique Errors (Oracle Accuracy)}

\section{How many times a candidate lost due to cycle prevention, and how many times this introduced an error?}

\section{Learning Curve}

\section{Conclusion}
\label{sec:concl}

\XXX On the other hand, we have shown that our hand-crafted word classes and some
additional data help Moses achieve significantly better results than reported
previously. Hierarchical decoder Joshua can capture word order even better than
Moses. Its results are always slightly better. And as far as we know, our
current results are the best that have been reported on this dataset.

\section*{Acknowledgements}

\XXX Acknowledgements: poděkovat Joakimovi a Ryanovi za zveřejnění parserů.
The research has been supported by the grant 
MSM0021620838 (Czech Ministry of Education).

\begin{small}
\bibliography{paper}
\end{small}

\end{document}
