% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode
% File acl-ijcnlp2009.tex
%
% Contact  jshin@csie.ncnu.edu.tw
%%
%% Based on the style files for EACL-2009 and IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl-ijcnlp2009}
\usepackage[	
   pdfdisplaydoctitle, breaklinks, colorlinks, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black, 
   backref, hyperfootnotes]{hyperref} % backref a modre URL asi nakonec zrusime 
%\usepackage{times}
\usepackage{url}
\usepackage{amsmath}
\usepackage{color} %pro korektury
\usepackage{paralist} % for better itemize and enumerate

% a footer required for the first page
\usepackage{fancyhdr}
\fancyhead{} % clear all header fields
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[C]{Proceedings of ICON-2009: 7th International Conference on Natural Language Processing, Macmillan Publishers, India. Also accessible from http://ltrc.iiit.ac.in/proceedings/ICON-2009}

% xelatex
\usepackage{fontspec, xunicode, xltxtra}
\defaultfontfeatures{Mapping=tex-text}
\setmainfont{Times New Roman}
\setmonofont[Scale=MatchLowercase]{Luxi Mono}
\setmathsf{Lohit Hindi}%\XXX
%\newfontinstance\hi[Script=Devanagari]{Lohit Hindi}
\newfontinstance\hifont[Script=Devanagari]{Code2000}
\newfontinstance\bnfont[Script=Bengali]{Code2000}
\newfontinstance\tefont[Script=Telugu]{Code2000}
\newfontinstance\translitfont{Gentium}
\newcommand{\hi}[1]{{\hifont #1}}
\newcommand{\bn}[1]{{\bnfont #1}}
\newcommand{\te}[1]{{\tefont #1}}
\newcommand{\translit}[1]{{\translitfont \textit{#1}}}

% natbib
\usepackage{natbib}
\bibliographystyle{plainnat}
\bibpunct{(}{)}{;}{a}{,}{,}

% our defs
\def\perscite#1{\citet{#1}}  
\def\parcite#1{\citep{#1}} 
%ps: Did you mean \citep and \citet (in-parentheses and textual reference)? There is even more, see `texdoc natbib`.
\def\Sref#1{Section~\ref{#1}}
\def\Tref#1{Table~\ref{#1}}
\def\Fref#1{Figure~\ref{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}} % komentare (TODO)
\newcommand{\XXX}{\textcolor{red}{XXX }} % komentare (TODO)

\def\microsection#1{{\bf #1.}}



\title{Maximum Spanning Malt: Hiring World's Leading Dependency Parsers to Plant Indian Trees%
% Tohle nechat zakomentované, je to jen tahák, jak udělat acknowledgement grantu při nedostatku místa. Jinak ale mám momentálně na konci opravdovou sekci Acknowledgements.
%\thanks{ \hspace{.6em}The research has been supported by the grant 
%MSM0021620838 (Czech Ministry of Education).}
}

% Tady je posuzování taky slepé? Až odtajním autora, nezapomenout odtajnit i acknowledgements!
\author{%Daniel Zeman\\
%Univerzita Karlova v Praze, Ústav formální a aplikované lingvistiky\\
%Malostranské náměstí 25, CZ-11800, Praha, Czechia\\ 
%\texttt{zeman@ufal.mff.cuni.cz}
}

%\title{Instructions for ACL-IJCNLP 2009 Proceedings}
%
%\author{First Author\\
%  Affiliation / Address line 1\\
%  Affiliation / Address line 2\\
%  {\tt email@domain}  \And
%  Second Author\\
%  Affiliation / Address line 1\\
%  Affiliation / Address line 2\\
%  {\tt  email@domain}}

\date{}

\begin{document}
\maketitle
\thispagestyle{fancy}

\begin{abstract}
We present our system used for participation in the ICON 2009 NLP Tools Contest: dependency parsing of Hindi, Bangla and Telugu. The system consists of three existing, freely available dependency parsers, two of which (MST and Malt) have been known to produce state-of-the-art structures on data sets for other languages. Various settings of the parsers are explored in order to adjust them for the three Indian languages, and a voting approach is used to combine them into a superparser. Since there is nothing novel about the approach used, substantial part of the paper is devoted to the analysis of errors the system makes on the given data sets.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Dependency parsing, i.e. sentence analysis that outputs tree of word-on-word dependencies (as opposed to constituent trees of context-free derivations), gained growing attention and popularity recently. There are data-driven dependency parsers that can be trained on syntactically annotated corpora (treebanks) and new, previously unseen material can be parsed very efficiently \citep{nivre:2009:ACLIJCNLP}.

Most of the successful parsers employ discriminative learning techniques to sort out vast sets of potentially useful features observed in the input text. Thus, for every new training treebank, smart feature engineering is the key to getting the most out of the existing parsers, regardless how well they performed on other data sets and languages. Now that there are new treebanks available for two Indo-Aryan and one Dravidian language, we took three existing dependency parsers and explored the possibilities of tuning them for the new training data. Both parser configuration and data preprocessing are relevant approaches to the tuning. In addition, we used parser combination to further improve the results.

Throughout the paper we focus mainly on the unlabeled attachment score. Although the parsers produce labeled dependencies, we do not optimize the system towards label accuracy.

The rest of the paper is organized as follows: In \Sref{sec:system}, we describe the respective parsers and the combined parsing system. In \Sref{sec:experiments}, we report on the experiments we performed, discuss various results on the development set and analyze the errors. In \Sref{sec:evaluation} we present the official results on the test data. We conclude by summarizing the best configuration we were able to find, and future implications.

\section{System Description}
\label{sec:system}

Several good trainable dependency parsers have emerged during the past five years. The CoNLL-X \citep{buchholz-marsi:2006:CoNLL-X} and CoNLL 2007 \citep{nivre-EtAl:2007:EMNLP-CoNLL2007} shared tasks in multilingual dependency parsing have greatly contributed to the development of the parsers. Some of the parsers are now freely available on the web, some are even open-source. We selected three of the publicly available parsers for our experiments:

%\microsection{MST Parser}
\subsection{MST Parser}
\label{sec:mst}
The Maximum Spanning Tree (MST) parser \citep{mst} views the sentence as an oriented complete graph with edges weighted by a feature scoring function. It finds for the graph a spanning tree that maximizes the weights of the edges. A multi-class classification algorithm called MIRA is used to compute the scoring function.

MST Parser achieved the best unlabeled attachment scores (UAS) for 9 out of the 13 languages of CoNLL-X, and second best scores in two others. Parsing is fast but training the parser takes many hours on large treebanks. On small data however, multiple quick experiments with different settings are still doable. The parser is implemented in Java and freely available for download.\footnote{\url{http://sourceforge.net/projects/mstparser/}}

%\microsection{Malt Parser}
\subsection{Malt Parser}
\label{sec:malt}
The Malt Parser  \citep{malt} is a deterministic shift-reduce parser where input words can be either put to the stack or taken from the stack and combined to form a dependency. The decision, which operation to perform, is made by an oracle based on various features of the words in the input buffer and the stack. The default machine learning algorithm used to train the oracle is a sort of SVN (support vector machine) classifier \citep{svm}.

Malt Parser has participated in both CoNLL-X and CoNLL 2007 shared tasks, and although it achieved the best UAS in three languages only, it usually scored among the five best parsers, sometimes with statistically insignificant difference from the winner. Malt Parser is really fast and its new Java implementation is open-source, freely available for download.\footnote{\url{http://maltparser.org/}}

%\microsection{DZ Parser}
\subsection{DZ Parser}
\label{sec:dz}
In order to combine the two above parsers, we needed a third parser. We picked DZ Parser \citep{dzparser}, which is also reasonably fast and freely available.\footnote{\url{http://ufal.mff.cuni.cz/~zeman/projekty/parser/}} Although its accuracy, if compared to MST or Malt, is worse by a wide margin, this parser proved useful because its only role was to help to form a majority whenever MST and Malt disagreed.

DZ Parser builds a model of bigrams of words that occur together in a dependency; most of the time, words are identified by their part of speech tags and morphological features. The parser was originally developed for Czech but it can be re-trained for any other language.\footnote{Of course there are other dependency parsers that successfully participated in the CoNLL shared tasks and are available for download. One alternative worth mentioning is the ISBN Parser \citep{titov-henderson:2007:EMNLP-CoNLL2007} at \url{http://flake.cs.uiuc.edu/~titov/}.}

\subsection{Voting Superparser}
\label{sec:voting}
The three parsers are combined using a simple weighted-voting approach similar to \citet{biblio:ZeZaImprovingParsing2005}, except that the output is guaranteed to be cycle-free. We start by evaluating every parser separately on the development data. The UAS of each parser is subsequently used as the weight of that parser's vote. Dependencies are parent-child relations, and for every node there are up to three candidates for its parent (if all three parsers disagree). Candidates get weighted votes -- e.g., if parsers with weights $w_1 = 0.8$ and $w_2 = 0.7$ agree on the candidate, the candidate gets 1.5 votes. Since we have only three parsers, in practice this means that the candidate of the best parser looses only if 1. the other two parsers agree on someone else, or 2. if attaching the child to this candidate would create a cycle.

The tree is constructed from the root down. We repeatedly add nodes whose winning parent candidates are already in the tree. If none of the remaining nodes meet this condition, we have to break a cycle. We do so by removing the candidate (for parentship of any remaining node) with least weighted votes. Then we go on with adding nodes until all nodes are attached or there is another cycle to break.

\section{Experiments}
\label{sec:experiments}

The final test data are blind, any error analysis is therefore impossible. That is why all scores given in this section were measured on the development data. All three treebanks follow the same annotation scheme and each of them is available in two flavors:

\begin{compactitem}
\item \textit{nomorph} variety contains word forms, chunk labels, dependency links and dependency labels
\item \textit{morph} variety is augmented by automatically assigned lemmas, part of speech tags and values of morphological features (gender, number, person, case, postposition and tam -- tense+aspect+modality)
\end{compactitem}

\Tref{tab:baseline} shows baseline results on the \textit{nomorph} data.

\begin{table}[ht]
\begin{centering}
%\small
\begin{tabular}{l|l|l|l}
& \textbf{MST} & \textbf{Malt} & \textbf{DZ} \\
\hline
hi & 80.32 & \textbf{81.84} & 62.00\\
bn & 82.00 & \textbf{84.71} & 71.02\\
te & 77.63 & \textbf{80.89} & 70.52\\
\end{tabular}
\caption{Baseline UAS of the three parsers on \textit{nomorph} development data. Language codes follow ISO 639: hi = Hindi, bn = Bangla, te = Telugu.}
\label{tab:baseline}
\end{centering}
\end{table}

\XXX what configuration of MST and Malt led to the baseline results?

Using the \textit{morph} data substantially deteriorates the accuracy. It could be caused by tagging errors (recall: morphological information has been disambiguated automatically) but data sparseness seems to be a more likely reason.

\begin{table}[ht]
\begin{centering}
%\small
\begin{tabular}{l|r|r|r|r|r|r}
& \textbf{occ} & \textbf{frm} & \textbf{lem} & \textbf{cl} & \textbf{pos} & \textbf{feat} \\
\hline
hi & 13779 & 3973 & 3134 & 10 & \XXX & 714\\
bn & 6449 & 2997 & 2336 & 14 & \XXX & 367\\
te & 5494 & 2462 & 1403 & 12 & \XXX & 453\\
\end{tabular}
\caption{Size of the training corpora: occ -- word occurrences, frm -- distinct forms, lem -- lemmas, cl -- chunk labels, pos -- part of speech tags, feat -- feature value combinations.}
\label{tab:corpus}
\end{centering}
\end{table}

%\begin{table}[ht]
%\begin{centering}
%\small
%\begin{tabular}{l|l|l|l}
%& \textbf{MST} & \textbf{Malt} & \textbf{DZ} \\
%\hline
%hi & - & \textbf{-} & 44.32\\
%bn & - & \textbf{-} & 50.31\\
%te & - & \textbf{-} & 36.89\\
%\end{tabular}
%\caption{UAS on morph data (POS tag and features concatenated to one tag).}
%\label{tab:baselinemorph}
%\end{centering}
%\end{table}

\begin{table}[ht]
\begin{centering}
%\small
\begin{tabular}{l|l|l|l}
& \textbf{MST} & \textbf{Malt} & \textbf{DZ} \\
\hline
hi & - & \textbf{-} & 44.32\\
bn & - & \textbf{-} & 50.31\\
te & - & \textbf{-} & 36.89\\
\end{tabular}
\caption{UAS on morph data (POS tag and features concatenated to one tag).}
\label{tab:poscasevib}
\end{centering}
\end{table}

\XXX Otevřít soubor eval.txt. Mám v něm další výsledky, zejména u všech parserů natrénování na pos+pád+vibhakti.

%\begin{compactitem}
%\item \hi{स्टैंडर्डज} \textit{(sṭaiṁḍarḍaja)} \translit{(sṭaiṁḍarḍaja)}
%\item \hi{स्टैंडर्डस} \textit{(sṭaiṁḍarḍasa)}
%\item \hi{स्टैंडर्ड्स} \textit{(sṭaiṁḍarḍsa)}
%\item \bn{সরিয়ে} Tohle je bengálsky.
%\item \te{వాడుతున్నాం} Tohle je telugsky.
%\end{compactitem}

\subsection{Nonprojectivity}
\label{sec:nonprojectivity}

Nonprojectivity is a property of the dependency structure and the word order \citep{neproj} that makes parsing more difficult. All three parsers can produce nonprojective structures and all three treebanks are nonprojective. However, except for Hindi, the proportion of nonprojective dependencies is so small that one can hardly imagine that running the parsers in nonprojective mode would bring any improvement. A quick experiment with Malt Parser switched to the nonprojective Stack method revealed that it actually hurts the results even for Hindi.

\begin{table}[ht]
\begin{centering}
%\small
\begin{tabular}{l|ll}
& \textbf{Edges} & \textbf{Sentences} \\
\hline
hi & 01.83 & 13.93\\
bn & 00.96 & 05.49\\
te & 00.45 & 01.31\\
\end{tabular}
\caption{Percentage of nonprojective dependencies, and of sentences containing at least one nonprojectivity.}
\label{tab:nonprojectivity}
\end{centering}
\end{table}

\subsection{Parser Unique Errors (Oracle Accuracy)}

\subsection{How many times a candidate lost due to cycle prevention, and how many times this introduced an error?}

\subsection{Learning Curve}

\section{Official Evaluation}
\label{sec:evaluation}

Finally, we present the official evaluation of our voting superparser, as measured by the organizers on the test data. For this purpose, the parsing system has been retrained on both the training data and the development data. The results are shown in \Tref{tab:evaluation}.

\begin{table}[ht]
\begin{centering}
\small
\begin{tabular}{l|l|l|l}
& \textbf{UAS} & \textbf{LAA} & \textbf{LAS} \\
\hline
hi & 88.58 (3:90.31) & 72.66 (4:76.38) & 68.60 (4:74.48)\\
bn & 86.06 (4:90.32) & 71.28 (4:81.27) & 66.70 (5:79.81)\\
te & 80.27 (4:86.28) & 54.20 (4:61.58) & 49.91 (4:60.55)\\
\end{tabular}
\caption{Official scores on the test data: unlabeled attachment score (UAS), label assignment accuracy (LAA) and labeled attachment score (LAS). The numbers in parentheses are the rank of our system and the score of the best system w.r.t. the given metric.}
\label{tab:evaluation}
\end{centering}
\end{table}

\section{Conclusion}
\label{sec:concl}

\XXX On the other hand, we have shown that our hand-crafted word classes and some
additional data help Moses achieve significantly better results than reported
previously. Hierarchical decoder Joshua can capture word order even better than
Moses. Its results are always slightly better. And as far as we know, our
current results are the best that have been reported on this dataset.

\section*{Acknowledgements}

\XXX Acknowledgements: poděkovat Joakimovi a Ryanovi za zveřejnění parserů.
The research has been supported by the grant 
MSM0021620838 (Czech Ministry of Education).

\begin{small}
\bibliography{paper}
\end{small}

\end{document}
